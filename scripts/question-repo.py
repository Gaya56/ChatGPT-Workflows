import openai 
import os
import time
import glob
import re
import fnmatch
from typing import List, Dict, Any

"""
question-repo.py
----------------
An interactive script that uses GPT to provide Q&A about your repository.
It references:
  - index.md        (generated by create-index.py in your repo root)
  - find-path.txt   (from openai-find-paths.py in the OpenAI folder)

It also supports scanning additional code for context if desired,
and can rotate through multiple API keys if rate-limited.

Usage:
  1. Place this script in your repository (e.g., scripts/question-repo.py).
  2. Update 'api_keys' with your own GPT/OpenAI keys (or use secrets).
  3. Run: python question-repo.py
  4. Interactively ask questions about the codebase structure, lines, or architecture.
"""

# List of API keys for potential rotation
api_keys = [
    "API_KEY_1_HERE",
    #"",
    #"",
]
current_key_index = 0

# Set up the initial OpenAI environment with the first key
os.environ["OPENAI_API_KEY"] = api_keys[current_key_index]
openai.api_key = api_keys[current_key_index]

# System prompt describing how GPT should respond
system_prompt = """You are an expert code analyst and documentation processor.
Respond to queries about this repository's structure, code files, architecture,
and recommended modifications. Provide specific line numbers or file paths
where possible, referencing context from index.md, find-path.txt, or scanned code.
Always maintain a helpful, organized tone with clear bullet points and references.
"""

# Initial user prompt, can be updated to reflect the goal of the script
initial_prompt = """Welcome to the repository Q&A! 
I have some code, plus an 'index.md' and a 'find-path.txt' describing files.
You can also scan a subset of code if needed. 
Please provide an overview of this codebase, referencing line numbers or file paths
whenever possible. Then I'll ask follow-up questions.
"""

def rotate_api_key() -> None:
    """
    Rotate to the next available API key in the list.
    """
    global current_key_index
    current_key_index = (current_key_index + 1) % len(api_keys)
    os.environ["OPENAI_API_KEY"] = api_keys[current_key_index]
    openai.api_key = api_keys[current_key_index]
    print(f"Switched to API key #{current_key_index + 1}")

def send_message_to_gpt(messages: List[Dict[str, Any]]) -> str:
    """
    Send messages to GPT and return the response, with automatic key rotation on failure.
    """
    global current_key_index
    
    # Attempt up to len(api_keys) times in case of rate limits
    for _ in range(len(api_keys)):
        try:
            completion = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=1500,  # Adjust as needed
                temperature=0.3
            )
            if completion.choices:
                return completion.choices[0].message.content
            else:
                return "No response received."
        except Exception as e:
            error_str = str(e).lower()
            if "rate limit" in error_str or "quota" in error_str or "capacity" in error_str:
                print(f"API key #{current_key_index + 1} hit rate limit. Rotating to next key...")
                rotate_api_key()
                time.sleep(1)
            else:
                return f"Error occurred while calling OpenAI API: {str(e)}"
    
    return "All available API keys are rate-limited or exhausted. Please try again later."

def print_with_formatting(text: str):
    """
    Print GPT responses with a clearer format (separator lines, etc.).
    """
    print("\n" + "="*80)
    print(text)
    print("="*80 + "\n")

def read_repo_context(index_path="index.md", find_path_file="OpenAI/find-path.txt") -> str:
    """
    Read the index.md and find-path.txt to form a base context about the repo.
    """
    context = ""
    
    if os.path.exists(index_path):
        try:
            with open(index_path, "r", encoding="utf-8", errors="ignore") as idx:
                context += "== index.md Content ==\n"
                context += idx.read() + "\n\n"
        except Exception as e:
            context += f"(Couldn't read index.md: {str(e)})\n\n"
    else:
        context += "(No index.md found)\n\n"
    
    if os.path.exists(find_path_file):
        try:
            with open(find_path_file, "r", encoding="utf-8", errors="ignore") as fpf:
                context += "== find-path.txt Content ==\n"
                context += fpf.read() + "\n\n"
        except Exception as e:
            context += f"(Couldn't read find-path.txt: {str(e)})\n\n"
    else:
        context += "(No find-path.txt found)\n\n"
    
    return context

def scan_additional_code(base_paths: List[str], max_files=3, max_chars=3000) -> str:
    """
    Optionally scan a few code files to add to context, from the base_paths if desired.
    This is similar to logic from gather-functions or question-based scanning.
    """
    # Basic approach: gather up to `max_files` from each path for demonstration
    # In real usage, refine or integrate with gather-functions approach.
    
    context = "== Additional Code Snippets ==\n\n"
    code_extensions = ["*.js", "*.jsx", "*.ts", "*.tsx", "*.svelte", "*.py"]
    exclude_dirs = ["*node_modules*", "*.git*", "*OpenAI*", "*__pycache__*"]
    all_collected = []
    
    for base_path in base_paths:
        for ext in code_extensions:
            pattern = os.path.join(base_path, "**", ext)
            candidates = glob.glob(pattern, recursive=True)
            # Filter out excluded
            filtered = []
            for c in candidates:
                skip = False
                for ex in exclude_dirs:
                    if fnmatch.fnmatch(c, ex):
                        skip = True
                        break
                if not skip:
                    filtered.append(c)
            # Take up to max_files from each path
            chosen = filtered[:max_files]
            all_collected.extend(chosen)
    
    # Deduplicate
    all_collected = sorted(set(all_collected))
    
    for file_path in all_collected:
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as cf:
                    content = cf.read(max_chars)
                    if len(content) >= max_chars:
                        content += "\n... [content truncated] ..."
                    context += f"FILE: {file_path}\n```\n{content}\n```\n\n"
            except Exception as e:
                context += f"FILE: {file_path}\n(Couldn't read: {str(e)})\n\n"
    
    return context

def main():
    # Step 1: Read the existing index.md and find-path.txt for base context
    repo_context = read_repo_context(index_path="index.md", find_path_file="OpenAI/find-path.txt")
    
    # Step 2: Optionally gather a few code snippets
    # (Adjust these paths or skip entirely if you just rely on gather-functions.)
    extra_code_context = scan_additional_code([
        "src",
        ".svelte-kit"
    ], max_files=2, max_chars=2000)
    
    # Combine them
    combined_context = repo_context + extra_code_context
    
    # Step 3: Create initial conversation
    conversation_history = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": initial_prompt + "\n\n---\n" + combined_context}
    ]
    
    print(f"Initialized question-repo script with {len(api_keys)} API keys.")
    print("Starting analysis. You can ask follow-up questions after the initial answer.\n")
    
    # Step 4: Get the initial GPT response
    response = send_message_to_gpt(conversation_history)
    conversation_history.append({"role": "assistant", "content": response})
    print_with_formatting(response)
    
    print("Ask follow-up questions or type 'exit' to quit.\n")
    while True:
        user_input = input("> ")
        if user_input.lower().strip() in ["exit", "quit"]:
            print("Exiting question-repo session.")
            break
        
        conversation_history.append({"role": "user", "content": user_input})
        print("Processing your query...\n")
        response = send_message_to_gpt(conversation_history)
        conversation_history.append({"role": "assistant", "content": response})
        print_with_formatting(response)

if __name__ == "__main__":
    main()